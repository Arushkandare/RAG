{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84a3f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain_community in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-google-genai in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: google-generativeai in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (0.8.5)\n",
      "Requirement already satisfied: chromadb in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (1.0.13)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (0.3.67)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain_community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain_community) (2.3.1)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (2.174.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: tqdm in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-generativeai) (4.14.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (6.0.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: anyio in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: sympy in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.33.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: filelock in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/arushkandare/Arush/New folder/RAG/.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain_community langchain-google-genai google-generativeai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396721bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyDGiDme3NtO47LmWgqv_Fr7UC908ybYaC0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6cccc",
   "metadata": {},
   "source": [
    "# Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf2eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Loading Docs\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Splitting Docs\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embedding\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "vectorstore = Chroma.from_documents(documents = splits, embedding = embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e6abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatGoogleGenerativeAI(model= \"models/gemini-1.5-flash\" , temperature = 0.3)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bb1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: How does climate change impact crop yields and farming practices globally?\n",
      "Query 2: \n",
      "Query 3: What are the specific agricultural challenges posed by rising temperatures, altered precipitation patterns, and increased frequency of extreme weather events?\n",
      "Query 4: \n",
      "Query 5: Discuss the economic and social consequences of climate change-induced disruptions to agricultural production and food security.\n",
      "Query 6: \n",
      "Query 7: Explore the adaptation and mitigation strategies being employed by farmers and policymakers to address the effects of climate change on agriculture.\n",
      "Query 8: \n",
      "Query 9: What scientific evidence exists to support the link between climate change and changes in agricultural productivity, including pest infestations and soil degradation?\n"
     ]
    }
   ],
   "source": [
    "queries = generate_queries.invoke({\"question\": \"What are the effects of climate change on agriculture?\"})\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"Query {i}: {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33ac4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"What is the task decompostition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({ \"question\": question })\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3eb5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM agents can decompose tasks in three ways:\n",
      "\n",
      "1.  **Simple prompting:**  The LLM is prompted with questions like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" to break down the task.\n",
      "\n",
      "2.  **Task-specific instructions:**  The task is decomposed using instructions tailored to the task type.  For example, \"Write a story outline\" for writing a novel.\n",
      "\n",
      "3.  **Human input:**  Humans can directly provide the task decomposition.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = \"models/gemini-1.5-flash\", temperature = 0) \n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638328d",
   "metadata": {},
   "source": [
    "# RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d84dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "058eeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGoogleGenerativeAI(model = \"models/gemini-1.5-flash\", temperature = 0)\n",
    "    | (lambda x: x.content.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0399e8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k = 60):\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key = lambda x: x[1], reverse = True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({ \"question\": question })\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5b0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM agents can decompose tasks in three ways:\n",
      "\n",
      "1.  **Simple prompting:**  Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"\n",
      "2.  **Task-specific instructions:** Providing instructions tailored to the task, such as \"Write a story outline.\" for writing a novel.\n",
      "3.  **Human input:**  Directly involving human users in the decomposition process.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0931521",
   "metadata": {},
   "source": [
    "# Decompostition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87cf4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905a4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = \"models/gemini-1.5-flash\", temperature = 0.3)\n",
    "\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | ( lambda x: x.split(\"\\n\") ) )\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({ \"question\": question })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf961de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to \"What are the main components of an LLM-powered autonomous agent system?\":',\n",
       " '',\n",
       " '1. **\"LLM agent architecture components\"**: This query focuses on the structural elements of the system, aiming to find diagrams and descriptions of the overall design.',\n",
       " '',\n",
       " '2. **\"Key modules of large language model agents\"**: This query seeks to identify the functional blocks or modules within the system, such as memory, planning, and action execution components.',\n",
       " '',\n",
       " '3. **\"Software components for building autonomous LLM agents\"**: This query is geared towards practical implementation, looking for information on specific software libraries, frameworks, or tools used in building such systems.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366ae2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fa4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = \"models/gemini-1.5-flash\", temperature = 0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a77abf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, there's no direct mention of specific software libraries, frameworks, or tools used for building autonomous LLM agents.  The documents focus on conceptual components and challenges, such as the unreliability of natural language interfaces and the need for tools and memory.\n",
      "\n",
      "However, we can infer some possibilities based on the examples given:\n",
      "\n",
      "* **AutoGPT:** This is explicitly mentioned as a proof-of-concept, implying it's a potential software tool or framework (though the provided text doesn't detail its underlying components).  The text highlights that a significant portion of its code deals with format parsing, suggesting a need for robust parsing libraries.\n",
      "\n",
      "* **ChatGPT Plugins and OpenAI API function calling:** These are cited as practical examples of LLMs augmented with tool use.  This implies the use of the OpenAI API and potentially libraries for interacting with it.  The existence of \"ChatGPT Plugins\" suggests a framework or system for managing and integrating external tools.\n",
      "\n",
      "* **Libraries for Natural Language Processing (NLP):**  Given the heavy reliance on natural language interfaces, various NLP libraries are likely involved.  However, the specific libraries aren't named.  These could include libraries for parsing, text processing, and potentially handling different language formats.\n",
      "\n",
      "* **Libraries for interacting with external systems:**  The ability to use tools like internet browsing, code execution, and API calls requires libraries for interacting with these external systems.  Again, the specific libraries are not mentioned.\n",
      "\n",
      "In summary, while the provided text doesn't offer a definitive list of software components, it points towards the need for tools and libraries related to:\n",
      "\n",
      "* **LLM interaction:**  OpenAI API, potentially other LLM APIs.\n",
      "* **Natural Language Processing:**  Parsing, text processing, potentially language-specific libraries.\n",
      "* **External System Interaction:**  Libraries for web scraping, code execution (potentially using Python's `subprocess` module or similar), and API interaction (using libraries like `requests`).\n",
      "* **Framework/System for managing tools:**  Something like the system underlying ChatGPT Plugins.\n",
      "\n",
      "Further research into AutoGPT and the OpenAI API would be necessary to identify the specific software components used in their implementations.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8107a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    sub_questions = sub_question_generator_chain.invoke({ \"question\": question })\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({ \"context\": retrieved_docs,\n",
    "                                                                \"question\": question })\n",
    "        rag_results.append(answer)\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04025d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An LLM-powered autonomous agent system comprises several key components working together.  The core is a Large Language Model (LLM) acting as the \"brain,\" responsible for planning and reacting to the environment.  This LLM interacts with the world and its internal state through a natural language interface, a crucial component whose reliability is a significant challenge.  The system also includes:\n",
      "\n",
      "* **Memory:**  For storing and retrieving information relevant to the agent's tasks and experiences.\n",
      "* **Tools:** External resources and APIs accessed via the natural language interface to perform actions and gather information.\n",
      "* **Planning System:**  A mechanism for decomposing complex tasks into smaller, manageable steps.  This often involves processing observations, generating questions, and translating reflections and environmental information into actionable commands.\n",
      "* **Mechanism for Processing Observations and Generating Questions:** This component allows the agent to actively sense and interpret its environment, formulating queries to gather more information as needed.\n",
      "* **System for Translating Reflections and Environmental Information into Actions:** This component bridges the gap between the LLM's internal reasoning and the external world, enabling the agent to execute its plans.\n",
      "* **User-provided Goals:** The system requires clearly defined objectives to guide its actions and decision-making.\n",
      "\n",
      "\n",
      "In short, the system integrates an LLM with mechanisms for perception, planning, action, and memory, all mediated by a natural language interface.  The robustness of this interface is a critical area for ongoing development.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\":context,\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc344a7a",
   "metadata": {},
   "source": [
    "# Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5b52243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc6c3565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the techniques for improving question answering in large language models?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatGoogleGenerativeAI(model = \"models/gemini-1.5-flash\", temperature = 0.3) | StrOutputParser()\n",
    "\n",
    "question = \"What is Query Translation for RAG systems?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95c3e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for Large Language Model (LLM) agents is the process of breaking down a large, complex task into smaller, more manageable sub-tasks or subgoals.  This allows the agent to handle complex requests more efficiently.  There are several methods for achieving this:\n",
      "\n",
      "1. **LLM-driven decomposition:** The LLM itself can decompose tasks through prompting techniques.  Simple prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\" can elicit a step-by-step breakdown.  Alternatively, task-specific instructions, such as \"Write a story outline\" for a novel-writing task, can guide the decomposition process.\n",
      "\n",
      "2. **Human-driven decomposition:**  A human can directly provide the decomposition of the task into sub-tasks.\n",
      "\n",
      "The provided text highlights a system where the LLM acts as a \"brain,\" parsing user requests into multiple tasks with attributes like task type, ID, dependencies, and arguments.  Few-shot examples are used to guide the LLM in this task planning and decomposition process.  The decomposition facilitates subsequent stages, such as model selection where the LLM chooses appropriate expert models for each sub-task.  The limitations of context length necessitate task type-based filtration in model selection.  Experiments show that even simple tasks like solving verbal math problems can be challenging due to the difficulty LLMs face in extracting the correct arguments from the problem statement.  This underscores the importance of effective task decomposition for successful LLM agent operation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "response = chain.invoke({ \"question\": question })\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadb661",
   "metadata": {},
   "source": [
    "# HYDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91cb2728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition, in the context of Large Language Model (LLM) agents, refers to the process of breaking down a complex, high-level task into a sequence of simpler, more manageable sub-tasks that are individually tractable for the LLM.  This decomposition is crucial because LLMs, while possessing impressive capabilities, often struggle with tasks requiring extensive reasoning, memory, or external interaction.  A single, monolithic prompt attempting to address a complex task may exceed the LLM's context window or lead to suboptimal performance due to the inherent limitations of its single-step reasoning.  Effective task decomposition mitigates these limitations by enabling the LLM to process information incrementally, leveraging its strengths in generating text and performing simpler operations.  The decomposed sub-tasks are typically designed to be self-contained, with clear inputs and outputs, facilitating sequential execution and potentially allowing for parallel processing where appropriate.  The choice of decomposition strategy significantly impacts the overall efficiency and success of the agent, with considerations including the granularity of sub-tasks, the dependencies between them, and the selection of appropriate tools or APIs for execution.  Ultimately, successful task decomposition transforms an intractable problem into a series of solvable steps, enabling LLMs to tackle significantly more complex tasks than would otherwise be possible.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatGoogleGenerativeAI(model = \"models/gemini-1.5-flash\", temperature = 0.3) | StrOutputParser() \n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({ \"question\": question })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c69933cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({ \"question\": question })\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dabbbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided text, task decomposition for LLM agents is the process of breaking down large tasks into smaller, manageable subgoals.  This can be achieved in several ways:  through simple prompting (e.g., \"Steps for XYZ\"), using task-specific instructions (e.g., \"Write a story outline\"), or with human input.  The goal is to enable efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({ \"context\": retrieved_docs, \"question\": question })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
